diff --git a/.idea/workspace.xml b/.idea/workspace.xml
index c8777f8..c03d10d 100644
--- a/.idea/workspace.xml
+++ b/.idea/workspace.xml
@@ -4,42 +4,20 @@
     <option name="autoReloadType" value="SELECTIVE" />
   </component>
   <component name="ChangeListManager">
-    <list default="true" id="ba02fceb-bbae-4e4f-be3a-d63ede9d4b8e" name="Changes" comment="some messy commit. Idk what im doin">
-      <change afterPath="$PROJECT_DIR$/RLOR/envs/heuristic_solver.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/envs/plot_env.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/envs/test_env.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710409176/events.out.tfevents.1710409176.VF-070237N.14872.0" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710409176/main.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710428847/events.out.tfevents.1710428847.VF-070237N.12572.0" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710428847/main.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710429062/events.out.tfevents.1710429062.VF-070237N.19792.0" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710429062/main.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710432982/events.out.tfevents.1710432982.VF-070237N.19680.0" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710432982/main.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710433072/events.out.tfevents.1710433072.VF-070237N.16668.0" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710433072/main.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710433462/events.out.tfevents.1710433462.VF-070237N.8388.0" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710433462/main.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710434559/events.out.tfevents.1710434559.VF-070237N.14472.0" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710434559/main.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710434688/events.out.tfevents.1710434688.VF-070237N.15076.0" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710434688/main.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710434756/events.out.tfevents.1710434756.VF-070237N.21492.0" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710434756/main.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710435293/events.out.tfevents.1710435293.VF-070237N.15888.0" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710435293/main.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710435644/events.out.tfevents.1710435644.VF-070237N.19736.0" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710435644/main.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710435695/events.out.tfevents.1710435695.VF-070237N.18544.0" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710435695/main.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710436362/events.out.tfevents.1710436362.VF-070237N.17292.0" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710436362/main.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710436403/events.out.tfevents.1710436403.VF-070237N.17496.0" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710436403/main.py" afterDir="false" />
+    <list default="true" id="ba02fceb-bbae-4e4f-be3a-d63ede9d4b8e" name="Changes" comment="commit for gitlab">
+      <change afterPath="$PROJECT_DIR$/runs/cvrp-v1__exp5_50_steps___1__1711111788/events.out.tfevents.1711111813.VF-070237N.20088.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/runs/cvrp-v1__exp5_50_steps___1__1711111788/main.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/runs/cvrp-v1__exp5_50_steps___1__1711112094/events.out.tfevents.1711112114.VF-070237N.18068.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/runs/cvrp-v1__exp5_50_steps___1__1711112094/main.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/runs/cvrp-v1__exp5_50_steps___1__1711112859/events.out.tfevents.1711112903.VF-070237N.18980.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/runs/cvrp-v1__exp5_50_steps___1__1711112859/main.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/wandb/debug-cli.dein_el.log" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/RLOR" beforeDir="false" afterPath="$PROJECT_DIR$/RLOR" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/RLOR/envs/cvrp_vector_env.py" beforeDir="false" afterPath="$PROJECT_DIR$/RLOR/envs/cvrp_vector_env.py" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/RLOR/envs/cvrp_vehfleet_env.py" beforeDir="false" afterPath="$PROJECT_DIR$/RLOR/envs/cvrp_vehfleet_env.py" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/RLOR/ppo_or.py" beforeDir="false" afterPath="$PROJECT_DIR$/RLOR/ppo_or.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/description.md" beforeDir="false" afterPath="$PROJECT_DIR$/description.md" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/rlor_vrp" beforeDir="false" afterPath="$PROJECT_DIR$/rlor_vrp" afterDir="false" />
     </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
@@ -82,15 +60,15 @@
     <option name="hideEmptyMiddlePackages" value="true" />
     <option name="showLibraryContents" value="true" />
   </component>
-  <component name="PropertiesComponent"><![CDATA[{
-  "keyToString": {
-    "ASKED_ADD_EXTERNAL_FILES": "true",
-    "RunOnceActivity.OpenProjectViewOnStart": "true",
-    "RunOnceActivity.ShowReadmeOnStart": "true",
-    "last_opened_file_path": "C:/Users/dein_el/PycharmProjects/rlor_vrp",
-    "settings.editor.selected.configurable": "project.propVCSSupport.DirectoryMappings"
+  <component name="PropertiesComponent">{
+  &quot;keyToString&quot;: {
+    &quot;ASKED_ADD_EXTERNAL_FILES&quot;: &quot;true&quot;,
+    &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,
+    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,
+    &quot;last_opened_file_path&quot;: &quot;C:/Users/dein_el/PycharmProjects/rlor_vrp&quot;,
+    &quot;settings.editor.selected.configurable&quot;: &quot;project.propVCSSupport.DirectoryMappings&quot;
   }
-}]]></component>
+}</component>
   <component name="RecentsManager">
     <key name="CopyFile.RECENT_KEYS">
       <recent name="C:\Users\dein_el\PycharmProjects\rlor_vrp\data" />
@@ -101,6 +79,27 @@
     </key>
   </component>
   <component name="RunManager" selected="Python.ppo_or">
+    <configuration name="heuristic_solver" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+      <module name="rlor_vrp" />
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs>
+        <env name="PYTHONUNBUFFERED" value="1" />
+      </envs>
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/RLOR/envs" />
+      <option name="IS_MODULE_SDK" value="true" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/RLOR/envs/heuristic_solver.py" />
+      <option name="PARAMETERS" value="" />
+      <option name="SHOW_COMMAND_LINE" value="false" />
+      <option name="EMULATE_TERMINAL" value="false" />
+      <option name="MODULE_MODE" value="false" />
+      <option name="REDIRECT_INPUT" value="false" />
+      <option name="INPUT_FILE" value="" />
+      <method v="2" />
+    </configuration>
     <configuration name="main" type="PythonConfigurationType" factoryName="Python" nameIsGenerated="true">
       <module name="rlor_vrp" />
       <option name="INTERPRETER_OPTIONS" value="" />
@@ -122,6 +121,27 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
+    <configuration name="plot_env" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+      <module name="rlor_vrp" />
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs>
+        <env name="PYTHONUNBUFFERED" value="1" />
+      </envs>
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/RLOR/envs" />
+      <option name="IS_MODULE_SDK" value="true" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/RLOR/envs/plot_env.py" />
+      <option name="PARAMETERS" value="" />
+      <option name="SHOW_COMMAND_LINE" value="false" />
+      <option name="EMULATE_TERMINAL" value="false" />
+      <option name="MODULE_MODE" value="false" />
+      <option name="REDIRECT_INPUT" value="false" />
+      <option name="INPUT_FILE" value="" />
+      <method v="2" />
+    </configuration>
     <configuration name="ppo_or" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="rlor_vrp" />
       <option name="INTERPRETER_OPTIONS" value="" />
@@ -146,6 +166,8 @@
     <recent_temporary>
       <list>
         <item itemvalue="Python.ppo_or" />
+        <item itemvalue="Python.heuristic_solver" />
+        <item itemvalue="Python.plot_env" />
       </list>
     </recent_temporary>
   </component>
@@ -172,7 +194,14 @@
       <option name="project" value="LOCAL" />
       <updated>1710277083266</updated>
     </task>
-    <option name="localTasksCounter" value="3" />
+    <task id="LOCAL-00003" summary="commit for gitlab">
+      <created>1710506842857</created>
+      <option name="number" value="00003" />
+      <option name="presentableId" value="LOCAL-00003" />
+      <option name="project" value="LOCAL" />
+      <updated>1710506842857</updated>
+    </task>
+    <option name="localTasksCounter" value="4" />
     <servers />
   </component>
   <component name="Vcs.Log.Tabs.Properties">
@@ -206,7 +235,8 @@
       <path value="$PROJECT_DIR$/rlor_vrp" />
     </ignored-roots>
     <MESSAGE value="Here I started to implement the vehicle fleet environment. Changed the context in RLOR, added the additional environment. The biggest issue is to implement def go_to() because it process the trajectories for ppo." />
-    <option name="LAST_COMMIT_MESSAGE" value="Here I started to implement the vehicle fleet environment. Changed the context in RLOR, added the additional environment. The biggest issue is to implement def go_to() because it process the trajectories for ppo." />
+    <MESSAGE value="commit for gitlab" />
+    <option name="LAST_COMMIT_MESSAGE" value="commit for gitlab" />
   </component>
   <component name="XDebuggerManager">
     <breakpoint-manager>
Submodule RLOR contains modified content
diff --git a/RLOR/envs/__pycache__/cvrp_vector_env.cpython-310.pyc b/RLOR/envs/__pycache__/cvrp_vector_env.cpython-310.pyc
new file mode 100644
index 0000000..10eb623
Binary files /dev/null and b/RLOR/envs/__pycache__/cvrp_vector_env.cpython-310.pyc differ
diff --git a/RLOR/envs/__pycache__/cvrp_vehfleet_env.cpython-310.pyc b/RLOR/envs/__pycache__/cvrp_vehfleet_env.cpython-310.pyc
new file mode 100644
index 0000000..ff096e7
Binary files /dev/null and b/RLOR/envs/__pycache__/cvrp_vehfleet_env.cpython-310.pyc differ
diff --git a/RLOR/envs/__pycache__/vrp_data.cpython-310.pyc b/RLOR/envs/__pycache__/vrp_data.cpython-310.pyc
new file mode 100644
index 0000000..503f5b2
Binary files /dev/null and b/RLOR/envs/__pycache__/vrp_data.cpython-310.pyc differ
diff --git a/RLOR/envs/cvrp_vehfleet_env.py b/RLOR/envs/cvrp_vehfleet_env.py
index b09223b..4b76e90 100644
--- a/RLOR/envs/cvrp_vehfleet_env.py
+++ b/RLOR/envs/cvrp_vehfleet_env.py
@@ -22,7 +22,7 @@ class CVRPFleetEnv(gym.Env):
         self.max_nodes = 50
         self.capacity_limit = 40
         self.max_num_vehicles = 5
-        self.n_traj = 2
+        self.n_traj = 50
         # if eval_data==True, load from 'test' set, the '0'th data
         self.eval_data = False
         self.eval_partition = "test"
@@ -72,8 +72,8 @@ class CVRPFleetEnv(gym.Env):
 
         #self.done = (action == 0) & self.is_all_visited()
 
-        print(f' dones? {self.done}')
-        print(f' nr_of_vehicles {self.num_veh}')
+        #print(f' dones? {self.done}')
+        #print(f' nr_of_vehicles {self.num_veh}')
         #print(f' is_all_visited {is_all_visited}')
         return self.state, self.reward, self.done, self.info
 
@@ -160,7 +160,7 @@ class CVRPFleetEnv(gym.Env):
         self.demands_with_depot = self.demands.copy()
 
     def _go_to(self, destination):
-        print(f'destinations {destination}')
+        #print(f'destinations {destination}')
         # if capacity is below the demand, return to depot, restart veh capacity and reduce the number of veh
 
         ### Ok, I guess I got it..action 21 for instance, this is basically node[destination-1] index in observation arrays. because 0-49 indexing  of arrays.
@@ -245,7 +245,7 @@ class CVRPFleetEnv(gym.Env):
         self.load[self.load<0] = 0
         self.num_veh[self.num_veh<=0] = 0
         self.reward = reward
-        print(f'rewards from go_to {reward} \n ')
+        #print(f'rewards from go_to {reward} \n ')
 
     def step(self, action):
         # return last state after done,
diff --git a/RLOR/models/__pycache__/attention_model_wrapper.cpython-310.pyc b/RLOR/models/__pycache__/attention_model_wrapper.cpython-310.pyc
new file mode 100644
index 0000000..cdbd185
Binary files /dev/null and b/RLOR/models/__pycache__/attention_model_wrapper.cpython-310.pyc differ
diff --git a/RLOR/models/nets/__pycache__/__init__.cpython-310.pyc b/RLOR/models/nets/__pycache__/__init__.cpython-310.pyc
new file mode 100644
index 0000000..2cd2392
Binary files /dev/null and b/RLOR/models/nets/__pycache__/__init__.cpython-310.pyc differ
diff --git a/RLOR/models/nets/attention_model/__pycache__/__init__.cpython-310.pyc b/RLOR/models/nets/attention_model/__pycache__/__init__.cpython-310.pyc
new file mode 100644
index 0000000..723f98c
Binary files /dev/null and b/RLOR/models/nets/attention_model/__pycache__/__init__.cpython-310.pyc differ
diff --git a/RLOR/models/nets/attention_model/__pycache__/attention_model.cpython-310.pyc b/RLOR/models/nets/attention_model/__pycache__/attention_model.cpython-310.pyc
new file mode 100644
index 0000000..96112b6
Binary files /dev/null and b/RLOR/models/nets/attention_model/__pycache__/attention_model.cpython-310.pyc differ
diff --git a/RLOR/models/nets/attention_model/__pycache__/context.cpython-310.pyc b/RLOR/models/nets/attention_model/__pycache__/context.cpython-310.pyc
new file mode 100644
index 0000000..79713e3
Binary files /dev/null and b/RLOR/models/nets/attention_model/__pycache__/context.cpython-310.pyc differ
diff --git a/RLOR/models/nets/attention_model/__pycache__/decoder.cpython-310.pyc b/RLOR/models/nets/attention_model/__pycache__/decoder.cpython-310.pyc
new file mode 100644
index 0000000..e1f3905
Binary files /dev/null and b/RLOR/models/nets/attention_model/__pycache__/decoder.cpython-310.pyc differ
diff --git a/RLOR/models/nets/attention_model/__pycache__/dynamic_embedding.cpython-310.pyc b/RLOR/models/nets/attention_model/__pycache__/dynamic_embedding.cpython-310.pyc
new file mode 100644
index 0000000..f46d595
Binary files /dev/null and b/RLOR/models/nets/attention_model/__pycache__/dynamic_embedding.cpython-310.pyc differ
diff --git a/RLOR/models/nets/attention_model/__pycache__/embedding.cpython-310.pyc b/RLOR/models/nets/attention_model/__pycache__/embedding.cpython-310.pyc
new file mode 100644
index 0000000..e9798ac
Binary files /dev/null and b/RLOR/models/nets/attention_model/__pycache__/embedding.cpython-310.pyc differ
diff --git a/RLOR/models/nets/attention_model/__pycache__/encoder.cpython-310.pyc b/RLOR/models/nets/attention_model/__pycache__/encoder.cpython-310.pyc
new file mode 100644
index 0000000..fce3057
Binary files /dev/null and b/RLOR/models/nets/attention_model/__pycache__/encoder.cpython-310.pyc differ
diff --git a/RLOR/models/nets/attention_model/__pycache__/multi_head_attention.cpython-310.pyc b/RLOR/models/nets/attention_model/__pycache__/multi_head_attention.cpython-310.pyc
new file mode 100644
index 0000000..cb3f689
Binary files /dev/null and b/RLOR/models/nets/attention_model/__pycache__/multi_head_attention.cpython-310.pyc differ
diff --git a/RLOR/ppo_or.py b/RLOR/ppo_or.py
index 07bf876..5795498 100644
--- a/RLOR/ppo_or.py
+++ b/RLOR/ppo_or.py
@@ -50,7 +50,7 @@ def parse_args():
         help="the weight decay of the optimizer")
     parser.add_argument("--num-envs", type=int, default=1024,
         help="the number of parallel game environments")
-    parser.add_argument("--num-steps", type=int, default=25,
+    parser.add_argument("--num-steps", type=int, default=100,
         help="the number of steps to run in each environment per policy rollout")
     parser.add_argument("--anneal-lr", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
         help="Toggle learning rate annealing for policy and value networks")
@@ -392,6 +392,6 @@ if __name__ == "__main__":
             writer.add_scalar("test/episodic_return_mean", avg_episodic_return, global_step)
             writer.add_scalar("test/episodic_return_max", max_episodic_return, global_step)
             writer.add_scalar("test/episodic_length", avg_episodic_length, global_step)
-
+    print("model trained")
     envs.close()
     writer.close()
diff --git a/RLOR/wrappers/__pycache__/recordWrapper.cpython-310.pyc b/RLOR/wrappers/__pycache__/recordWrapper.cpython-310.pyc
new file mode 100644
index 0000000..c157444
Binary files /dev/null and b/RLOR/wrappers/__pycache__/recordWrapper.cpython-310.pyc differ
diff --git a/RLOR/wrappers/__pycache__/syncVectorEnvPomo.cpython-310.pyc b/RLOR/wrappers/__pycache__/syncVectorEnvPomo.cpython-310.pyc
new file mode 100644
index 0000000..4069948
Binary files /dev/null and b/RLOR/wrappers/__pycache__/syncVectorEnvPomo.cpython-310.pyc differ
diff --git a/description.md b/description.md
index b93812d..6e520f9 100644
--- a/description.md
+++ b/description.md
@@ -10,6 +10,7 @@ Here the authors also stated, the RL platforms handle end-2-end NCO different. R
 While original KOOL implementation only adjust the decoder wights with REINFORCE.
 However, when we use PPO, especially in CleanRL, we work with Trajectories. And this is a trick.
 CleanRL-> https://docs.cleanrl.dev/
+but the idea of learning on trajectories in POMO is from here: https://arxiv.org/pdf/2010.16011.pdf
 For PPO best practices, tricks and implementation read carefully: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
 In CleanRL these trajectories are directly implemented in the environment, whereby in RLLIB for example, we can use standart, simple environment without trajectories view.
 It SIMPLIFIES a lot! because the trajectories in the environment is a big pain and costs me 3 days to understand and adjust only one function -> go_to(self) in the cvrp_vehfleet_env
@@ -231,7 +232,7 @@ ____
 NEXT STEPS:
 
 TODO:
-* Write and check the environment! Write a small class and check and visualize how the environment is working only with 1 trajectory!
+*[Done] Write and check the environment! Write a small class and check and visualize how the environment is working only with 1 trajectory!
 * Try to deploy and train the model on cluster! Use some parallel computing and so on! 
 * Check the model and compare with the RLLIB
 * Check, if the agent behaves properly. If not, look in cvrp_flet_env class. I suppose, that the agent will pursuit to reduce the number of vehicles as quick as possible, to achieve minim reward! 
Submodule rlor_vrp 396e075..e563ebc:
diff --git a/rlor_vrp/.idea/workspace.xml b/rlor_vrp/.idea/workspace.xml
new file mode 100644
index 0000000..8f701b6
--- /dev/null
+++ b/rlor_vrp/.idea/workspace.xml
@@ -0,0 +1,319 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="AutoImportSettings">
+    <option name="autoReloadType" value="SELECTIVE" />
+  </component>
+  <component name="ChangeListManager">
+    <list default="true" id="ba02fceb-bbae-4e4f-be3a-d63ede9d4b8e" name="Changes" comment="some messy commit. Idk what im doin">
+      <change afterPath="$PROJECT_DIR$/RLOR/envs/heuristic_solver.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/envs/plot_env.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/envs/test_env.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710409176/events.out.tfevents.1710409176.VF-070237N.14872.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710409176/main.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710428847/events.out.tfevents.1710428847.VF-070237N.12572.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710428847/main.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710429062/events.out.tfevents.1710429062.VF-070237N.19792.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710429062/main.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710432982/events.out.tfevents.1710432982.VF-070237N.19680.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710432982/main.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710433072/events.out.tfevents.1710433072.VF-070237N.16668.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710433072/main.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710433462/events.out.tfevents.1710433462.VF-070237N.8388.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710433462/main.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710434559/events.out.tfevents.1710434559.VF-070237N.14472.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710434559/main.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710434688/events.out.tfevents.1710434688.VF-070237N.15076.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710434688/main.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710434756/events.out.tfevents.1710434756.VF-070237N.21492.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710434756/main.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710435293/events.out.tfevents.1710435293.VF-070237N.15888.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710435293/main.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710435644/events.out.tfevents.1710435644.VF-070237N.19736.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710435644/main.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710435695/events.out.tfevents.1710435695.VF-070237N.18544.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710435695/main.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710436362/events.out.tfevents.1710436362.VF-070237N.17292.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710436362/main.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710436403/events.out.tfevents.1710436403.VF-070237N.17496.0" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/RLOR/runs/cvrp-v1__ppo_or__1__1710436403/main.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/RLOR" beforeDir="false" afterPath="$PROJECT_DIR$/RLOR" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/RLOR/envs/cvrp_vector_env.py" beforeDir="false" afterPath="$PROJECT_DIR$/RLOR/envs/cvrp_vector_env.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/RLOR/envs/cvrp_vehfleet_env.py" beforeDir="false" afterPath="$PROJECT_DIR$/RLOR/envs/cvrp_vehfleet_env.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/RLOR/ppo_or.py" beforeDir="false" afterPath="$PROJECT_DIR$/RLOR/ppo_or.py" afterDir="false" />
+    </list>
+    <option name="SHOW_DIALOG" value="false" />
+    <option name="HIGHLIGHT_CONFLICTS" value="true" />
+    <option name="HIGHLIGHT_NON_ACTIVE_CHANGELIST" value="false" />
+    <option name="LAST_RESOLUTION" value="IGNORE" />
+  </component>
+  <component name="ChangesViewManager">
+    <option name="groupingKeys">
+      <option value="directory" />
+      <option value="repository" />
+    </option>
+  </component>
+  <component name="FileTemplateManagerImpl">
+    <option name="RECENT_TEMPLATES">
+      <list>
+        <option value="Python Script" />
+      </list>
+    </option>
+  </component>
+  <component name="Git.Settings">
+    <branch-grouping>
+      <option value="directory" />
+      <option value="repository" />
+    </branch-grouping>
+    <option name="PREVIEW_PUSH_PROTECTED_ONLY" value="true" />
+    <option name="ROOT_SYNC" value="SYNC" />
+  </component>
+  <component name="HighlightingSettingsPerFile">
+    <setting file="file://$USER_HOME$/Anaconda3/envs/rlor_vrp/Lib/site-packages/gym/envs/registration.py" root0="SKIP_INSPECTION" />
+    <setting file="file://$USER_HOME$/Anaconda3/envs/rlor_vrp/Lib/site-packages/numpy/core/arrayprint.py" root0="SKIP_INSPECTION" />
+  </component>
+  <component name="MarkdownSettingsMigration">
+    <option name="stateVersion" value="1" />
+  </component>
+  <component name="ProjectId" id="2cSCENABZmKuZFBjnbizfCBtUD6" />
+  <component name="ProjectLevelVcsManager" settingsEditedManually="true">
+    <ConfirmationsSetting value="2" id="Add" />
+  </component>
+  <component name="ProjectViewState">
+    <option name="hideEmptyMiddlePackages" value="true" />
+    <option name="showLibraryContents" value="true" />
+  </component>
+  <component name="PropertiesComponent"><![CDATA[{
+  "keyToString": {
+    "ASKED_ADD_EXTERNAL_FILES": "true",
+    "RunOnceActivity.OpenProjectViewOnStart": "true",
+    "RunOnceActivity.ShowReadmeOnStart": "true",
+    "last_opened_file_path": "C:/Users/dein_el/PycharmProjects/rlor_vrp/data",
+    "settings.editor.selected.configurable": "project.propVCSSupport.DirectoryMappings"
+  }
+}]]></component>
+  <component name="RecentsManager">
+    <key name="CopyFile.RECENT_KEYS">
+      <recent name="C:\Users\dein_el\PycharmProjects\rlor_vrp\data" />
+      <recent name="C:\Users\dein_el\PycharmProjects\rlor_vrp" />
+    </key>
+    <key name="MoveFile.RECENT_KEYS">
+      <recent name="C:\Users\dein_el\PycharmProjects\rlor_vrp\RLOR\envs" />
+    </key>
+  </component>
+  <component name="RunManager" selected="Python.ppo_or">
+    <configuration name="main" type="PythonConfigurationType" factoryName="Python" nameIsGenerated="true">
+      <module name="rlor_vrp" />
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs>
+        <env name="PYTHONUNBUFFERED" value="1" />
+      </envs>
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$" />
+      <option name="IS_MODULE_SDK" value="true" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/main.py" />
+      <option name="PARAMETERS" value="" />
+      <option name="SHOW_COMMAND_LINE" value="false" />
+      <option name="EMULATE_TERMINAL" value="false" />
+      <option name="MODULE_MODE" value="false" />
+      <option name="REDIRECT_INPUT" value="false" />
+      <option name="INPUT_FILE" value="" />
+      <method v="2" />
+    </configuration>
+    <configuration name="ppo_or" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+      <module name="rlor_vrp" />
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs>
+        <env name="PYTHONUNBUFFERED" value="1" />
+      </envs>
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/RLOR" />
+      <option name="IS_MODULE_SDK" value="true" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/RLOR/ppo_or.py" />
+      <option name="PARAMETERS" value="" />
+      <option name="SHOW_COMMAND_LINE" value="false" />
+      <option name="EMULATE_TERMINAL" value="false" />
+      <option name="MODULE_MODE" value="false" />
+      <option name="REDIRECT_INPUT" value="false" />
+      <option name="INPUT_FILE" value="" />
+      <method v="2" />
+    </configuration>
+    <recent_temporary>
+      <list>
+        <item itemvalue="Python.ppo_or" />
+      </list>
+    </recent_temporary>
+  </component>
+  <component name="SpellCheckerSettings" RuntimeDictionaries="0" Folders="0" CustomDictionaries="0" DefaultDictionary="application-level" UseSingleDictionary="true" transferred="true" />
+  <component name="TaskManager">
+    <task active="true" id="Default" summary="Default task">
+      <changelist id="ba02fceb-bbae-4e4f-be3a-d63ede9d4b8e" name="Changes" comment="" />
+      <created>1708095480286</created>
+      <option name="number" value="Default" />
+      <option name="presentableId" value="Default" />
+      <updated>1708095480286</updated>
+    </task>
+    <task id="LOCAL-00001" summary="Here I started to implement the vehicle fleet environment. Changed the context in RLOR, added the additional environment. The biggest issue is to implement def go_to() because it process the trajectories for ppo.">
+      <created>1710276793747</created>
+      <option name="number" value="00001" />
+      <option name="presentableId" value="LOCAL-00001" />
+      <option name="project" value="LOCAL" />
+      <updated>1710276793747</updated>
+    </task>
+    <task id="LOCAL-00002" summary="Here I started to implement the vehicle fleet environment. Changed the context in RLOR, added the additional environment. The biggest issue is to implement def go_to() because it process the trajectories for ppo.">
+      <created>1710277083266</created>
+      <option name="number" value="00002" />
+      <option name="presentableId" value="LOCAL-00002" />
+      <option name="project" value="LOCAL" />
+      <updated>1710277083266</updated>
+    </task>
+    <option name="localTasksCounter" value="3" />
+    <servers />
+  </component>
+  <component name="Vcs.Log.Tabs.Properties">
+    <option name="TAB_STATES">
+      <map>
+        <entry key="MAIN">
+          <value>
+            <State>
+              <option name="FILTERS">
+                <map>
+                  <entry key="branch">
+                    <value>
+                      <list>
+                        <option value="main" />
+                      </list>
+                    </value>
+                  </entry>
+                </map>
+              </option>
+            </State>
+          </value>
+        </entry>
+      </map>
+    </option>
+  </component>
+  <component name="VcsManagerConfiguration">
+    <option name="ADD_EXTERNAL_FILES_SILENTLY" value="true" />
+    <ignored-roots>
+      <path value="$PROJECT_DIR$" />
+      <path value="$PROJECT_DIR$/RLOR" />
+      <path value="$PROJECT_DIR$/rlor_vrp" />
+    </ignored-roots>
+    <MESSAGE value="Here I started to implement the vehicle fleet environment. Changed the context in RLOR, added the additional environment. The biggest issue is to implement def go_to() because it process the trajectories for ppo." />
+    <option name="LAST_COMMIT_MESSAGE" value="Here I started to implement the vehicle fleet environment. Changed the context in RLOR, added the additional environment. The biggest issue is to implement def go_to() because it process the trajectories for ppo." />
+  </component>
+  <component name="XDebuggerManager">
+    <breakpoint-manager>
+      <breakpoints>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/main.py</url>
+          <line>8</line>
+          <option name="timeStamp" value="1" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/RLOR/ppo_or.py</url>
+          <line>342</line>
+          <option name="timeStamp" value="8" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/RLOR/ppo_or.py</url>
+          <line>273</line>
+          <option name="timeStamp" value="9" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/RLOR/ppo_or.py</url>
+          <line>235</line>
+          <option name="timeStamp" value="10" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/RLOR/models/attention_model_wrapper.py</url>
+          <line>171</line>
+          <option name="timeStamp" value="41" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/RLOR/models/nets/attention_model/context.py</url>
+          <line>89</line>
+          <option name="timeStamp" value="42" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/RLOR/models/nets/attention_model/multi_head_attention.py</url>
+          <line>80</line>
+          <option name="timeStamp" value="50" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/RLOR/models/nets/attention_model/multi_head_attention.py</url>
+          <line>83</line>
+          <option name="timeStamp" value="52" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/RLOR/ppo_or.py</url>
+          <line>223</line>
+          <option name="timeStamp" value="53" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/RLOR/models/attention_model_wrapper.py</url>
+          <line>122</line>
+          <option name="timeStamp" value="54" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/RLOR/models/attention_model_wrapper.py</url>
+          <line>147</line>
+          <option name="timeStamp" value="57" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/RLOR/envs/cvrp_vector_env.py</url>
+          <line>139</line>
+          <option name="timeStamp" value="68" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/RLOR/envs/cvrp_vector_env.py</url>
+          <line>129</line>
+          <option name="timeStamp" value="73" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/RLOR/envs/cvrp_vector_env.py</url>
+          <line>92</line>
+          <option name="timeStamp" value="76" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/RLOR/envs/cvrp_vector_env.py</url>
+          <line>80</line>
+          <option name="timeStamp" value="77" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/RLOR/envs/cvrp_vehfleet_env.py</url>
+          <line>82</line>
+          <option name="timeStamp" value="89" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/RLOR/envs/cvrp_vehfleet_env.py</url>
+          <line>180</line>
+          <option name="timeStamp" value="94" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/RLOR/envs/cvrp_vehfleet_env.py</url>
+          <line>86</line>
+          <option name="timeStamp" value="101" />
+        </line-breakpoint>
+      </breakpoints>
+      <default-breakpoints>
+        <breakpoint type="python-exception">
+          <properties notifyOnTerminate="true" exception="BaseException">
+            <option name="notifyOnTerminate" value="true" />
+          </properties>
+        </breakpoint>
+      </default-breakpoints>
+    </breakpoint-manager>
+    <watches-manager>
+      <configuration name="PythonConfigurationType">
+        <watch expression="self" />
+      </configuration>
+    </watches-manager>
+  </component>
+</project>
\ No newline at end of file
Submodule RLOR 0000000...83d17e6 (new submodule)
diff --git a/rlor_vrp/data/vrp100_test_seed1234.pkl b/rlor_vrp/data/vrp100_test_seed1234.pkl
new file mode 100644
index 0000000..22c176d
Binary files /dev/null and b/rlor_vrp/data/vrp100_test_seed1234.pkl differ
diff --git a/rlor_vrp/data/vrp100_validation_seed4321.pkl b/rlor_vrp/data/vrp100_validation_seed4321.pkl
new file mode 100644
index 0000000..22c176d
Binary files /dev/null and b/rlor_vrp/data/vrp100_validation_seed4321.pkl differ
diff --git a/rlor_vrp/data/vrp20_test_seed1234.pkl b/rlor_vrp/data/vrp20_test_seed1234.pkl
new file mode 100644
index 0000000..b4b23e1
Binary files /dev/null and b/rlor_vrp/data/vrp20_test_seed1234.pkl differ
diff --git a/rlor_vrp/data/vrp20_validation_seed4321.pkl b/rlor_vrp/data/vrp20_validation_seed4321.pkl
new file mode 100644
index 0000000..b4b23e1
Binary files /dev/null and b/rlor_vrp/data/vrp20_validation_seed4321.pkl differ
diff --git a/rlor_vrp/data/vrp50_test_seed1234.pkl b/rlor_vrp/data/vrp50_test_seed1234.pkl
new file mode 100644
index 0000000..fd4d13c
Binary files /dev/null and b/rlor_vrp/data/vrp50_test_seed1234.pkl differ
diff --git a/rlor_vrp/data/vrp50_validation_seed4321.pkl b/rlor_vrp/data/vrp50_validation_seed4321.pkl
new file mode 100644
index 0000000..fd4d13c
Binary files /dev/null and b/rlor_vrp/data/vrp50_validation_seed4321.pkl differ
diff --git a/rlor_vrp/description.md b/rlor_vrp/description.md
new file mode 100644
index 0000000..b93812d
--- /dev/null
+++ b/rlor_vrp/description.md
@@ -0,0 +1,252 @@
+## What has been done and what was the purpose here!
+
+Date: 13.03.2024
++ here I implement the additional environment with finite vehicle fleet.
+the issue was to adapt the environment for #ppo, so I can learn the policy with CleanRL framework.
+
+The authors of RLOR https://arxiv.org/pdf/2303.13117.pdf state, that CleanRL was the best RL library for them. They tried RLLIB and other libraries, but CleanRL won
+Why I need this? Well, first I want to compare the result of original RLOR code with my experiment in RLLIB.
+Here the authors also stated, the RL platforms handle end-2-end NCO different. RL platforms make the total loop training encoder AND decoder, while train.
+While original KOOL implementation only adjust the decoder wights with REINFORCE.
+However, when we use PPO, especially in CleanRL, we work with Trajectories. And this is a trick.
+CleanRL-> https://docs.cleanrl.dev/
+For PPO best practices, tricks and implementation read carefully: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
+In CleanRL these trajectories are directly implemented in the environment, whereby in RLLIB for example, we can use standart, simple environment without trajectories view.
+It SIMPLIFIES a lot! because the trajectories in the environment is a big pain and costs me 3 days to understand and adjust only one function -> go_to(self) in the cvrp_vehfleet_env
+For comparison, when using the RLLib, I need only define my simple problem environment, and the rest, i.e. parallellisation and vectorizing the environment  does RLLib under the hood
+
+for RLLib documentation read also this https://openreview.net/pdf?id=trNDfee72NQ
+
+Lets take a look on the environment. There are many issues with that!
+First, you need to understand the concept of batches (like many parallel environment) and trajectories (like simultaneous actions in one timestep in the same environment)
+First, we have to recall, that the PPO need trajectories, and they are somehow tricky to understand.
+For example
+
+ `![img.png](img.png "trajectori views")
+
+so, as you see, in ppo_or.py algorithm, we sample in each 
+
+    '''
+     ALGO LOGIC: action logic
+                with torch.no_grad():
+                    action, logprob, _, value, _ = agent.get_action_and_value_cached(
+                        next_obs, state=encoder_state
+                    )
+                    action = action.view(args.num_envs, args.n_traj)"
+    '''
+
+so the action becomes an 1D array of the shape (35,) when nr_trajectories in env is 35
+
+next, we neet to adapt the environment respectively. For instance THE cvrp_vehfleet_env.py class:
+we have some data_objects, like #nodes, #action_mask, #depot and #demand, which are the same for each batch.
+(However, the demand is altered in each move! Still struggling to understand, why we do not copy it with each trajectory)
+
+So, some data objects are of the shape [50] (nodes for example), the mask has the shape (35,51) where 35 are trajectories and 51 are nodes with depot
+demand has size 50 (nr of nodes) but no trajectories...
+
+However, self.done are trajectories (size 35), as well as actions (35 actions in each time step)
+load.size() -> (35,);
+last.size() -> (35,)
+AND hence, => nr_veh (35,)
+
+    '''
+        self.num_veh = np.array([self.max_num_vehicles] * self.n_traj)
+        self.done = np.array([False] * self.n_traj)
+    '''
+
+This basically says, that in each step, we  proceed different trajectories, some of them can end in dones soner, other later. 
+The batch start to learn, when all trajectories dones are TRUE! 
+
+____
+
+Now, lets take a look on the go_to() function (causes the most pain!)
+
+    '''
+    def _go_to(self, destination):
+        '''
+
+The theory behind this was, to first identify the indexes for allowable actions, nodes that can be reached. 
+And the not reachable actions, like when destination is already 0 or the demand exceed the vehicle load! 
+
+    ''' 
+        print(f'destinations {destination}')
+        # if capacity is below the demand, return to depot, restart veh capacity and reduce the number of veh
+
+        ### Ok, I guess I got it..action 21 for instance, this is basically node[destination-1] index in observation arrays. because 0-49 indexing  of arrays.
+        # This is due to the dimension mismatch. We have indicies from 1 to 50. and the indexes 0-49 in the demands. We need to substract 1 in order to match arrays..
+        any_depot_destinations = destination > 0
+        # because actions are calculated from 51 array with depot. and here we wantaccess the nodes f the 50 size, without depot
+        demands_indices = destination[destination > 0] - 1 # the same as list comprehension. Substract 1 from all destinations idx, when they are not zero (deopt).
+
+        demand_exceed_capacity = self.load[any_depot_destinations] < self.demands[demands_indices] # if there any depot in trajectories, this becomes array.size<n_traj
+        # Indices for operations
+        destination_zero_condition = destination == 0
+        true_indices_ = np.where(demand_exceed_capacity )[0]   # Indices where condition is true
+        false_indices_ = np.where(~demand_exceed_capacity)[0]  # Indices where condition is false
+
+        destination_zero_ = np.where(destination == 0)[0]
+        destination_not_zero_ = np.where(destination != 0)[0]
+
+        true_indices = np.unique(np.concatenate((true_indices_, destination_zero_)))
+
+        #if no depot destination and no exceeding capacity. just revert the indices array where these conditions are true.
+        false_indices = np.setdiff1d(np.arange(len(destination)),true_indices)
+
+        reward = np.zeros(len(destination))
+    ''' 
+
+
+when this condition is true, then return the vehicle back to the depot: 
+
+    '''
+        if len(true_indices)>0:
+            dest_not_reached = destination[true_indices]
+            self.num_veh[true_indices] -= 1
+
+            last_node = self.nodes[self.last[true_indices]]
+            dest_depot = np.zeros_like(dest_not_reached)
+            depot_node = self.nodes[dest_depot]
+            dist = self.cost(depot_node, last_node)
+
+            self.last[true_indices] = dest_not_reached
+
+            #set depot to true
+            self.visited[true_indices, dest_not_reached] = True
+
+            #set the destination to False aka not reached
+            #self.visited[true_indices, dest_not_reached] = True
+
+            destination[true_indices] = 0
+            reward[true_indices] = -dist
+    '''
+
+Note, how we indexing and selecting the trajectories!
+For example, 
++ **dest_not_reached = destination[true_indices]** selects all actions *destination* where the nodes can not be reached
+
+* then, we reduce the num_veh `self.num_veh[true_indices] -= 1` 
+
+* we assign the array of last nodes of the shape (35,) th same last nodes from previous step
+`last_node = self.nodes[self.last[true_indices]]`. Why? because we either didn't reach this node because of capacity constrains, OR the action is already 0
+
+* then we calculate the distance between last nodes and the depot `dest_depot = np.zeros_like(dest_not_reached)
+            ; depot_node = self.nodes[dest_depot]; 
+            dist = self.cost(depot_node, last_node)` 
+* and assign destinations only to the not_reached nodes , `self.last[true_indices] = dest_not_reached` (note, I changed this line to `self.last[true_indices] = dest_depot` because if the destination is zero, so the last node is also zero. and if it was not reached and returned to depot, then the last node is also zero. so in both cases we have zeros)
+* the next line sets all visited nodes to True. In this case, only depot nodes `self.visited[true_indices, dest_depot] = True`
+* at the end, when we return to depot, we set the actions to zero manually. And assign reward only to processed trajectories!
+
+
+Next we do similarly to the trajectories, which were reached and where the vehicle moved from A to B. 
+ 
+    '''
+        if len(false_indices)>0:
+            dest_next = destination[false_indices]
+            dest_node = self.nodes[dest_next]
+            dest_depot = np.zeros_like(dest_next)
+
+            dist = self.cost(dest_node, self.nodes[self.last[false_indices]])
+
+            self.last[false_indices] = dest_next
+
+            self.load[false_indices] -= self.demands[dest_next-1]
+
+            self.demands_with_depot[dest_next-1] = 0
+
+            #set destination to True, aka reached
+            self.visited[false_indices, dest_next] = True
+
+            # unmask the depot
+            self.visited[false_indices, dest_depot] = False
+
+            reward[false_indices] = -dist
+    '''
+
+* the steps are similar, only that we assign to the last node the destinations `self.last[false_indices] = dest_next`
+* and substract the demand from the load `self.load[false_indices] -= self.demands[dest_next-1]` Here is little bit unclear for me, WHY we substract all trajectories from the same load array. SO if we have different actions at the same time, can we substract from the same load different nodes many times from different trajectories? and when so, how to handle it, is it correct? IDK
+* also we unmask the depot. 
+* the line `self.demands_with_depot[dest_next-1] = 0` looks unnecessary to me. but however  
+
+
+At the very end I do.
+
+    '''
+        self.load[destination == 0] = 1
+        self.load[self.load<0] = 0
+        self.num_veh[self.num_veh<=0] = 0
+        self.reward = reward
+    '''
+..for all indices, for consistency
+
+_______________________
+
+I also added `self.no_other_vehicles()` in `def _update_mask(self):` in order to MASK the whole row of all nodes, when the number of vehicles in specific trajectory is 0! so it is additional check for this trajectory
+
+CHECK THIS!
+Maybe I will rise the problem, that the Agent will try to force the full usage of vehicles, because then it has less tours to drive, less milage and hence, bigger reward!
+CHECK THIS!
+
+Finally, because the all_visited function checks all visited nodes in trajectories and nodes (visited array has the shape (35,51)), the episode is done, where all nodes are visited. And they are visited, when there are no vehicles in trajectory!
+
+_____
+
+The last major adjustment was context class.
+I also added 
+    
+    '''
+    def _state_embedding(self, embeddings, state):
+        state_embedding = -state.used_capacity[:, :, None]
+        vehicles = state.get_num_veh()
+        state_embedding = torch.cat(( state_embedding,vehicles[:,:, None]),-1) #(1024,35,2)
+        return state_embedding
+    '''
+in context.py to include the vehicle constrain in the context state for decoder..
+
+AND the class 
+**class CVRPFleetEmbeddings(nn.Module):**
+
+    '''
+    def __init__(self, embedding_dim):
+        super(CVRPFleetEmbeddings, self).__init__()
+        node_dim = 3  # x, y, demand
+        scalar = 1
+
+        self.context_dim = embedding_dim + 2  # Embedding of last node + remaining_capacity AND num_vehicles
+        self.init_embed = nn.Linear(node_dim, embedding_dim)
+        self.init_embed_depot = nn.Linear(2, embedding_dim)  # depot embedding PLUS num of vehicles as the 3 feature
+
+    def forward(self, input):  # dict of 'loc', 'demand', 'depot', num_vehicles
+        # batch, 1, 2 -> batch, 1, embedding_dim
+        depot_embedding = self.init_embed_depot(input["depot"])[:, None, :]
+
+        node_embeddings = self.init_embed(
+            torch.cat((input["loc"], input["demand"][:, :, None]), -1)
+        )
+        out = torch.cat((depot_embedding, node_embeddings), 1)
+        return out
+    '''
+____
+
+
+NEXT STEPS:
+
+TODO:
+* Write and check the environment! Write a small class and check and visualize how the environment is working only with 1 trajectory!
+* Try to deploy and train the model on cluster! Use some parallel computing and so on! 
+* Check the model and compare with the RLLIB
+* Check, if the agent behaves properly. If not, look in cvrp_flet_env class. I suppose, that the agent will pursuit to reduce the number of vehicles as quick as possible, to achieve minim reward! 
+* 
+compare the vrp solution with heuristic in python using this lib:
+https://pyvrp.readthedocs.io/en/latest/examples/basic_vrps.html
+
+Date: 14.03.2024
+
+some improvements. Added the fixcosts when the vehicles return to depot in go_to().
+`reward[true_indices[return_to_depot_idx]] = -(dist[return_to_depot_idx] + 1)`
+- 1 is a fix costs for vehicles. CHECK. 
+- implemented additional filter, if the vehicle is staying in depot, do not assign reward!! 
+-
+Realized validation i.e. Heuristic approach and plotting of the problem!
+[heuristic_solver.py](RLOR%2Fenvs%2Fheuristic_solver.py)
+[plot_env.py](RLOR%2Fenvs%2Fplot_env.py)
+[test_env.py](RLOR%2Fenvs%2Ftest_env.py)
\ No newline at end of file
diff --git a/rlor_vrp/img.png b/rlor_vrp/img.png
new file mode 100644
index 0000000..3813340
Binary files /dev/null and b/rlor_vrp/img.png differ
diff --git a/rlor_vrp/main.py b/rlor_vrp/main.py
new file mode 100644
index 0000000..76d0e8c
--- /dev/null
+++ b/rlor_vrp/main.py
@@ -0,0 +1,16 @@
+# This is a sample Python script.
+
+# Press Umschalt+F10 to execute it or replace it with your code.
+# Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.
+
+
+def print_hi(name):
+    # Use a breakpoint in the code line below to debug your script.
+    print(f'Hi, {name}')  # Press Strg+F8 to toggle the breakpoint.
+
+
+# Press the green button in the gutter to run the script.
+if __name__ == '__main__':
+    print_hi('PyCharm')
+
+# See PyCharm help at https://www.jetbrains.com/help/pycharm/
Submodule rlor_vrp 0000000...396e075 (new submodule)
diff --git a/rlor_vrp/train_rlor_model.py b/rlor_vrp/train_rlor_model.py
new file mode 100644
index 0000000..e69de29
diff --git a/runs/cvrp-v1__exp5_50_steps___1__1711111788/events.out.tfevents.1711111813.VF-070237N.20088.0 b/runs/cvrp-v1__exp5_50_steps___1__1711111788/events.out.tfevents.1711111813.VF-070237N.20088.0
new file mode 100644
index 0000000..ba1552b
Binary files /dev/null and b/runs/cvrp-v1__exp5_50_steps___1__1711111788/events.out.tfevents.1711111813.VF-070237N.20088.0 differ
diff --git a/runs/cvrp-v1__exp5_50_steps___1__1711111788/main.py b/runs/cvrp-v1__exp5_50_steps___1__1711111788/main.py
new file mode 100644
index 0000000..b396b35
--- /dev/null
+++ b/runs/cvrp-v1__exp5_50_steps___1__1711111788/main.py
@@ -0,0 +1,397 @@
+# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppopy
+
+import argparse
+import os
+import random
+import shutil
+import time
+from distutils.util import strtobool
+
+import gym
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.optim as optim
+from torch.utils.tensorboard import SummaryWriter
+
+
+def parse_args():
+    # fmt: off
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
+        help="the name of this experiment")
+    parser.add_argument("--seed", type=int, default=1,
+        help="seed of the experiment")
+    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="if toggled, `torch.backends.cudnn.deterministic=False`")
+    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="if toggled, cuda will be enabled by default")
+    parser.add_argument("--track", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
+        help="if toggled, this experiment will be tracked with Weights and Biases")
+    parser.add_argument("--wandb-project-name", type=str, default="cleanRL",
+        help="the wandb's project name")
+    parser.add_argument("--wandb-entity", type=str, default=None,
+        help="the entity (team) of wandb's project")
+    parser.add_argument("--capture-video", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
+        help="whether to capture videos of the agent performances (check out `videos` folder)")
+
+    # Algorithm specific arguments
+    parser.add_argument("--problem", type=str, default="cvrp_fleet",
+        help="the OR problem we are trying to solve, it will be passed to the agent")
+    parser.add_argument("--env-id", type=str, default="cvrp-v1",
+        help="the id of the environment")
+    parser.add_argument("--env-entry-point", type=str, default="envs.cvrp_vehfleet_env:CVRPFleetEnv",
+        help="the path to the definition of the environment, for example `envs.cvrp_vector_env:CVRPVectorEnv` if the `CVRPVectorEnv` class is defined in ./envs/cvrp_vector_env.py")
+    parser.add_argument("--total-timesteps", type=int, default=6_000_000_000,
+        help="total timesteps of the experiments")
+    parser.add_argument("--learning-rate", type=float, default=1e-3,
+        help="the learning rate of the optimizer")
+    parser.add_argument("--weight-decay", type=float, default=0,
+        help="the weight decay of the optimizer")
+    parser.add_argument("--num-envs", type=int, default=1024,
+        help="the number of parallel game environments")
+    parser.add_argument("--num-steps", type=int, default=100,
+        help="the number of steps to run in each environment per policy rollout")
+    parser.add_argument("--anneal-lr", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="Toggle learning rate annealing for policy and value networks")
+    parser.add_argument("--gamma", type=float, default=0.99,
+        help="the discount factor gamma")
+    parser.add_argument("--gae-lambda", type=float, default=0.95,
+        help="the lambda for the general advantage estimation")
+    parser.add_argument("--num-minibatches", type=int, default=8,
+        help="the number of mini-batches")
+    parser.add_argument("--update-epochs", type=int, default=2,
+        help="the K epochs to update the policy")
+    parser.add_argument("--norm-adv", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="Toggles advantages normalization")
+    parser.add_argument("--clip-coef", type=float, default=0.2,
+        help="the surrogate clipping coefficient")
+    parser.add_argument("--clip-vloss", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="Toggles whether or not to use a clipped loss for the value function, as per the paper.")
+    parser.add_argument("--ent-coef", type=float, default=0.01,
+        help="coefficient of the entropy")
+    parser.add_argument("--vf-coef", type=float, default=0.5,
+        help="coefficient of the value function")
+    parser.add_argument("--max-grad-norm", type=float, default=0.5,
+        help="the maximum norm for the gradient clipping")
+    parser.add_argument("--target-kl", type=float, default=None,
+        help="the target KL divergence threshold")
+    parser.add_argument("--n-traj", type=int, default=50,
+        help="number of trajectories in a vectorized sub-environment")
+    parser.add_argument("--n-test", type=int, default=1000,
+        help="how many test instance")
+    parser.add_argument("--multi-greedy-inference", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="whether to use multiple trajectory greedy inference")
+    args = parser.parse_args()
+    args.batch_size = int(args.num_envs * args.num_steps)
+    args.minibatch_size = int(args.batch_size // args.num_minibatches)
+    # fmt: on
+    return args
+
+
+from wrappers.recordWrapper import RecordEpisodeStatistics
+
+
+def make_env(env_id, seed, cfg={}):
+    def thunk():
+        env = gym.make(env_id, **cfg)
+        env = RecordEpisodeStatistics(env)
+        env.seed(seed)
+        env.action_space.seed(seed)
+        env.observation_space.seed(seed)
+        return env
+
+    return thunk
+
+
+from models.attention_model_wrapper import Agent
+from wrappers.syncVectorEnvPomo import SyncVectorEnv
+
+if __name__ == "__main__":
+    args = parse_args()
+    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
+    if args.track:
+        import wandb
+
+        wandb.init(
+            project=args.wandb_project_name,
+            entity=args.wandb_entity,
+            sync_tensorboard=True,
+            config=vars(args),
+            name=run_name,
+            monitor_gym=True,
+            save_code=True,
+        )
+    writer = SummaryWriter(f"runs/{run_name}")
+    writer.add_text(
+        "hyperparameters",
+        "|param|value|\n|-|-|\n%s"
+        % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
+    )
+    os.makedirs(os.path.join(f"runs/{run_name}", "ckpt"), exist_ok=True)
+    shutil.copy(__file__, os.path.join(f"runs/{run_name}", "main.py"))
+    # TRY NOT TO MODIFY: seeding
+    random.seed(args.seed)
+    np.random.seed(args.seed)
+    torch.manual_seed(args.seed)
+    torch.backends.cudnn.deterministic = args.torch_deterministic
+
+    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
+
+    #######################
+    #### Env defintion ####
+    #######################
+
+    gym.envs.register(
+        id=args.env_id,
+        entry_point=args.env_entry_point,
+    )
+
+    # training env setup
+    envs = SyncVectorEnv([make_env(args.env_id, args.seed + i) for i in range(args.num_envs)])
+    # evaluation env setup: 1.) from a fix dataset, or 2.) generated with seed
+   # 1.) use test instance from a fix dataset
+    test_envs = SyncVectorEnv(
+        [
+            make_env(
+                args.env_id,
+                args.seed + i,
+                cfg={"eval_data": True, "eval_partition": "eval", "eval_data_idx": i},
+            )
+            for i in range(args.n_test)
+        ]
+    )
+#     # 2.) use generated evaluation instance instead
+#     import logging
+#     logging.warning('Using generated evaluation instance. For benchmarking, please download the fix dataset.')
+#     test_envs = SyncVectorEnv([make_env(args.env_id, args.seed + args.num_envs + i) for i in range(args.n_test)])
+    
+    assert isinstance(
+        envs.single_action_space, gym.spaces.MultiDiscrete
+    ), "only discrete action space is supported"
+
+    #######################
+    ### Agent defintion ###
+    #######################
+
+    agent = Agent(device=device, name=args.problem).to(device)
+    # agent.backbone.load_state_dict(torch.load('./vrp50.pt'))
+    optimizer = optim.Adam(
+        agent.parameters(), lr=args.learning_rate, eps=1e-5, weight_decay=args.weight_decay
+    )
+
+    #######################
+    # Algorithm defintion #
+    #######################
+
+    # ALGO Logic: Storage setup
+    obs = [None] * args.num_steps
+    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(
+        device
+    )
+    logprobs = torch.zeros((args.num_steps, args.num_envs, args.n_traj)).to(device)
+    rewards = torch.zeros((args.num_steps, args.num_envs, args.n_traj)).to(device)
+    dones = torch.zeros((args.num_steps, args.num_envs, args.n_traj)).to(device)
+    values = torch.zeros((args.num_steps, args.num_envs, args.n_traj)).to(device)
+
+    # TRY NOT TO MODIFY: start the game
+    global_step = 0
+    start_time = time.time()
+    next_obs = envs.reset()
+    next_done = torch.zeros(args.num_envs, args.n_traj).to(device)
+    num_updates = args.total_timesteps // args.batch_size
+    for update in range(1, num_updates + 1):
+        agent.train()
+        # Annealing the rate if instructed to do so.
+        if args.anneal_lr:
+            frac = 1.0 - (update - 1.0) / num_updates
+            lrnow = frac * args.learning_rate
+            optimizer.param_groups[0]["lr"] = lrnow
+        next_obs = envs.reset()
+        encoder_state = agent.backbone.encode(next_obs)
+        next_done = torch.zeros(args.num_envs, args.n_traj).to(device)
+        r = []
+        for step in range(0, args.num_steps):
+            global_step += 1 * args.num_envs
+            obs[step] = next_obs
+            dones[step] = next_done
+
+            # ALGO LOGIC: action logic
+            with torch.no_grad():
+                action, logprob, _, value, _ = agent.get_action_and_value_cached(
+                    next_obs, state=encoder_state
+                )
+                action = action.view(args.num_envs, args.n_traj)
+                values[step] = value.view(args.num_envs, args.n_traj)
+            actions[step] = action
+            logprobs[step] = logprob.view(args.num_envs, args.n_traj)
+            # TRY NOT TO MODIFY: execute the game and log data.
+            next_obs, reward, done, info = envs.step(action.cpu().numpy())
+            rewards[step] = torch.tensor(reward).to(device)
+            next_obs, next_done = next_obs, torch.Tensor(done).to(device)
+
+            for item in info:
+                if "episode" in item.keys():
+                    r.append(item)
+        print("completed_episodes=", len(r))
+        avg_episodic_return = np.mean([rollout["episode"]["r"].mean() for rollout in r])
+        max_episodic_return = np.mean([rollout["episode"]["r"].max() for rollout in r])
+        avg_episodic_length = np.mean([rollout["episode"]["l"].mean() for rollout in r])
+        print(
+            f"[Train] global_step={global_step}\n \
+            avg_episodic_return={avg_episodic_return}\n \
+            max_episodic_return={max_episodic_return}\n \
+            avg_episodic_length={avg_episodic_length}"
+        )
+        writer.add_scalar("charts/episodic_return_mean", avg_episodic_return, global_step)
+        writer.add_scalar("charts/episodic_return_max", max_episodic_return, global_step)
+        writer.add_scalar("charts/episodic_length", avg_episodic_length, global_step)
+        # bootstrap value if not done
+        with torch.no_grad():
+            next_value = agent.get_value_cached(next_obs, encoder_state).squeeze(-1)  # B x T
+            advantages = torch.zeros_like(rewards).to(device)  # steps x B x T
+            lastgaelam = torch.zeros(args.num_envs, args.n_traj).to(device)  # B x T
+            for t in reversed(range(args.num_steps)):
+                if t == args.num_steps - 1:
+                    nextnonterminal = 1.0 - next_done  # next_done: B
+                    nextvalues = next_value  # B x T
+                else:
+                    nextnonterminal = 1.0 - dones[t + 1]
+                    nextvalues = values[t + 1]  # B x T
+                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
+                advantages[t] = lastgaelam = (
+                    delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
+                )
+            returns = advantages + values
+
+        # flatten the batch
+        b_obs = {
+            k: np.concatenate([obs_[k] for obs_ in obs]) for k in envs.single_observation_space
+        }
+
+        # Edited
+        b_logprobs = logprobs.reshape(-1, args.n_traj)
+        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
+        b_advantages = advantages.reshape(-1, args.n_traj)
+        b_returns = returns.reshape(-1, args.n_traj)
+        b_values = values.reshape(-1, args.n_traj)
+
+        # Optimizing the policy and value network
+        assert args.num_envs % args.num_minibatches == 0
+        envsperbatch = args.num_envs // args.num_minibatches
+        envinds = np.arange(args.num_envs)
+        flatinds = np.arange(args.batch_size).reshape(args.num_steps, args.num_envs)
+
+        clipfracs = []
+        for epoch in range(args.update_epochs):
+            np.random.shuffle(envinds)
+            for start in range(0, args.num_envs, envsperbatch):
+                end = start + envsperbatch
+                mbenvinds = envinds[start:end]  # mini batch env id
+                mb_inds = flatinds[:, mbenvinds].ravel()  # be really careful about the index
+                r_inds = np.tile(np.arange(envsperbatch), args.num_steps)
+
+                cur_obs = {k: v[mbenvinds] for k, v in obs[0].items()}
+                encoder_state = agent.backbone.encode(cur_obs)
+                _, newlogprob, entropy, newvalue, _ = agent.get_action_and_value_cached(
+                    {k: v[mb_inds] for k, v in b_obs.items()},
+                    b_actions.long()[mb_inds],
+                    (embedding[r_inds, :] for embedding in encoder_state),
+                )
+                # _, newlogprob, entropy, newvalue = agent.get_action_and_value({k:v[mb_inds] for k,v in b_obs.items()}, b_actions.long()[mb_inds])
+                logratio = newlogprob - b_logprobs[mb_inds]
+                ratio = logratio.exp()
+
+                with torch.no_grad():
+                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
+                    old_approx_kl = (-logratio).mean()
+                    approx_kl = ((ratio - 1) - logratio).mean()
+                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
+
+                mb_advantages = b_advantages[mb_inds]
+                if args.norm_adv:
+                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (
+                        mb_advantages.std() + 1e-8
+                    )
+
+                # Policy loss
+                pg_loss1 = -mb_advantages * ratio
+                pg_loss2 = -mb_advantages * torch.clamp(
+                    ratio, 1 - args.clip_coef, 1 + args.clip_coef
+                )
+                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
+                # Value loss
+                newvalue = newvalue.view(-1, args.n_traj)
+                if args.clip_vloss:
+                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
+                    v_clipped = b_values[mb_inds] + torch.clamp(
+                        newvalue - b_values[mb_inds],
+                        -args.clip_coef,
+                        args.clip_coef,
+                    )
+                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
+                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
+                    v_loss = 0.5 * v_loss_max.mean()
+                else:
+                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
+
+                entropy_loss = entropy.mean()
+                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
+
+                optimizer.zero_grad()
+                loss.backward()
+                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
+                optimizer.step()
+
+            if args.target_kl is not None:
+                if approx_kl > args.target_kl:
+                    break
+
+        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
+        var_y = np.var(y_true)
+        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
+
+        # TRY NOT TO MODIFY: record rewards for plotting purposes
+        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
+        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
+        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
+        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
+        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
+        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
+        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
+        writer.add_scalar("losses/explained_variance", explained_var, global_step)
+        print("SPS:", int(global_step / (time.time() - start_time)))
+        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
+        if update % 1000 == 0 or update == num_updates:
+            torch.save(agent.state_dict(), f"runs/{run_name}/ckpt/{update}.pt")
+        if update % 100 == 0 or update == num_updates:
+            agent.eval()
+            test_obs = test_envs.reset()
+            r = []
+            for step in range(0, args.num_steps):
+                # ALGO LOGIC: action logic
+                with torch.no_grad():
+                    action, logits = agent(test_obs)
+                if step == 0:
+                    if args.multi_greedy_inference:
+                        if args.problem == 'tsp':
+                            action = torch.arange(args.n_traj).repeat(args.n_test, 1)
+                        elif args.problem == 'cvrp':
+                            action = torch.arange(1, args.n_traj + 1).repeat(args.n_test, 1)
+                # TRY NOT TO MODIFY: execute the game and log data.
+                test_obs, _, _, test_info = test_envs.step(action.cpu().numpy())
+
+                for item in test_info:
+                    if "episode" in item.keys():
+                        r.append(item)
+
+            avg_episodic_return = np.mean([rollout["episode"]["r"].mean() for rollout in r])
+            max_episodic_return = np.mean([rollout["episode"]["r"].max() for rollout in r])
+            avg_episodic_length = np.mean([rollout["episode"]["l"].mean() for rollout in r])
+            print(f"[test] episodic_return={max_episodic_return}")
+            writer.add_scalar("test/episodic_return_mean", avg_episodic_return, global_step)
+            writer.add_scalar("test/episodic_return_max", max_episodic_return, global_step)
+            writer.add_scalar("test/episodic_length", avg_episodic_length, global_step)
+
+    envs.close()
+    writer.close()
diff --git a/runs/cvrp-v1__exp5_50_steps___1__1711112094/events.out.tfevents.1711112114.VF-070237N.18068.0 b/runs/cvrp-v1__exp5_50_steps___1__1711112094/events.out.tfevents.1711112114.VF-070237N.18068.0
new file mode 100644
index 0000000..01c2db3
Binary files /dev/null and b/runs/cvrp-v1__exp5_50_steps___1__1711112094/events.out.tfevents.1711112114.VF-070237N.18068.0 differ
diff --git a/runs/cvrp-v1__exp5_50_steps___1__1711112094/main.py b/runs/cvrp-v1__exp5_50_steps___1__1711112094/main.py
new file mode 100644
index 0000000..b396b35
--- /dev/null
+++ b/runs/cvrp-v1__exp5_50_steps___1__1711112094/main.py
@@ -0,0 +1,397 @@
+# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppopy
+
+import argparse
+import os
+import random
+import shutil
+import time
+from distutils.util import strtobool
+
+import gym
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.optim as optim
+from torch.utils.tensorboard import SummaryWriter
+
+
+def parse_args():
+    # fmt: off
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
+        help="the name of this experiment")
+    parser.add_argument("--seed", type=int, default=1,
+        help="seed of the experiment")
+    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="if toggled, `torch.backends.cudnn.deterministic=False`")
+    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="if toggled, cuda will be enabled by default")
+    parser.add_argument("--track", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
+        help="if toggled, this experiment will be tracked with Weights and Biases")
+    parser.add_argument("--wandb-project-name", type=str, default="cleanRL",
+        help="the wandb's project name")
+    parser.add_argument("--wandb-entity", type=str, default=None,
+        help="the entity (team) of wandb's project")
+    parser.add_argument("--capture-video", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
+        help="whether to capture videos of the agent performances (check out `videos` folder)")
+
+    # Algorithm specific arguments
+    parser.add_argument("--problem", type=str, default="cvrp_fleet",
+        help="the OR problem we are trying to solve, it will be passed to the agent")
+    parser.add_argument("--env-id", type=str, default="cvrp-v1",
+        help="the id of the environment")
+    parser.add_argument("--env-entry-point", type=str, default="envs.cvrp_vehfleet_env:CVRPFleetEnv",
+        help="the path to the definition of the environment, for example `envs.cvrp_vector_env:CVRPVectorEnv` if the `CVRPVectorEnv` class is defined in ./envs/cvrp_vector_env.py")
+    parser.add_argument("--total-timesteps", type=int, default=6_000_000_000,
+        help="total timesteps of the experiments")
+    parser.add_argument("--learning-rate", type=float, default=1e-3,
+        help="the learning rate of the optimizer")
+    parser.add_argument("--weight-decay", type=float, default=0,
+        help="the weight decay of the optimizer")
+    parser.add_argument("--num-envs", type=int, default=1024,
+        help="the number of parallel game environments")
+    parser.add_argument("--num-steps", type=int, default=100,
+        help="the number of steps to run in each environment per policy rollout")
+    parser.add_argument("--anneal-lr", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="Toggle learning rate annealing for policy and value networks")
+    parser.add_argument("--gamma", type=float, default=0.99,
+        help="the discount factor gamma")
+    parser.add_argument("--gae-lambda", type=float, default=0.95,
+        help="the lambda for the general advantage estimation")
+    parser.add_argument("--num-minibatches", type=int, default=8,
+        help="the number of mini-batches")
+    parser.add_argument("--update-epochs", type=int, default=2,
+        help="the K epochs to update the policy")
+    parser.add_argument("--norm-adv", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="Toggles advantages normalization")
+    parser.add_argument("--clip-coef", type=float, default=0.2,
+        help="the surrogate clipping coefficient")
+    parser.add_argument("--clip-vloss", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="Toggles whether or not to use a clipped loss for the value function, as per the paper.")
+    parser.add_argument("--ent-coef", type=float, default=0.01,
+        help="coefficient of the entropy")
+    parser.add_argument("--vf-coef", type=float, default=0.5,
+        help="coefficient of the value function")
+    parser.add_argument("--max-grad-norm", type=float, default=0.5,
+        help="the maximum norm for the gradient clipping")
+    parser.add_argument("--target-kl", type=float, default=None,
+        help="the target KL divergence threshold")
+    parser.add_argument("--n-traj", type=int, default=50,
+        help="number of trajectories in a vectorized sub-environment")
+    parser.add_argument("--n-test", type=int, default=1000,
+        help="how many test instance")
+    parser.add_argument("--multi-greedy-inference", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="whether to use multiple trajectory greedy inference")
+    args = parser.parse_args()
+    args.batch_size = int(args.num_envs * args.num_steps)
+    args.minibatch_size = int(args.batch_size // args.num_minibatches)
+    # fmt: on
+    return args
+
+
+from wrappers.recordWrapper import RecordEpisodeStatistics
+
+
+def make_env(env_id, seed, cfg={}):
+    def thunk():
+        env = gym.make(env_id, **cfg)
+        env = RecordEpisodeStatistics(env)
+        env.seed(seed)
+        env.action_space.seed(seed)
+        env.observation_space.seed(seed)
+        return env
+
+    return thunk
+
+
+from models.attention_model_wrapper import Agent
+from wrappers.syncVectorEnvPomo import SyncVectorEnv
+
+if __name__ == "__main__":
+    args = parse_args()
+    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
+    if args.track:
+        import wandb
+
+        wandb.init(
+            project=args.wandb_project_name,
+            entity=args.wandb_entity,
+            sync_tensorboard=True,
+            config=vars(args),
+            name=run_name,
+            monitor_gym=True,
+            save_code=True,
+        )
+    writer = SummaryWriter(f"runs/{run_name}")
+    writer.add_text(
+        "hyperparameters",
+        "|param|value|\n|-|-|\n%s"
+        % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
+    )
+    os.makedirs(os.path.join(f"runs/{run_name}", "ckpt"), exist_ok=True)
+    shutil.copy(__file__, os.path.join(f"runs/{run_name}", "main.py"))
+    # TRY NOT TO MODIFY: seeding
+    random.seed(args.seed)
+    np.random.seed(args.seed)
+    torch.manual_seed(args.seed)
+    torch.backends.cudnn.deterministic = args.torch_deterministic
+
+    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
+
+    #######################
+    #### Env defintion ####
+    #######################
+
+    gym.envs.register(
+        id=args.env_id,
+        entry_point=args.env_entry_point,
+    )
+
+    # training env setup
+    envs = SyncVectorEnv([make_env(args.env_id, args.seed + i) for i in range(args.num_envs)])
+    # evaluation env setup: 1.) from a fix dataset, or 2.) generated with seed
+   # 1.) use test instance from a fix dataset
+    test_envs = SyncVectorEnv(
+        [
+            make_env(
+                args.env_id,
+                args.seed + i,
+                cfg={"eval_data": True, "eval_partition": "eval", "eval_data_idx": i},
+            )
+            for i in range(args.n_test)
+        ]
+    )
+#     # 2.) use generated evaluation instance instead
+#     import logging
+#     logging.warning('Using generated evaluation instance. For benchmarking, please download the fix dataset.')
+#     test_envs = SyncVectorEnv([make_env(args.env_id, args.seed + args.num_envs + i) for i in range(args.n_test)])
+    
+    assert isinstance(
+        envs.single_action_space, gym.spaces.MultiDiscrete
+    ), "only discrete action space is supported"
+
+    #######################
+    ### Agent defintion ###
+    #######################
+
+    agent = Agent(device=device, name=args.problem).to(device)
+    # agent.backbone.load_state_dict(torch.load('./vrp50.pt'))
+    optimizer = optim.Adam(
+        agent.parameters(), lr=args.learning_rate, eps=1e-5, weight_decay=args.weight_decay
+    )
+
+    #######################
+    # Algorithm defintion #
+    #######################
+
+    # ALGO Logic: Storage setup
+    obs = [None] * args.num_steps
+    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(
+        device
+    )
+    logprobs = torch.zeros((args.num_steps, args.num_envs, args.n_traj)).to(device)
+    rewards = torch.zeros((args.num_steps, args.num_envs, args.n_traj)).to(device)
+    dones = torch.zeros((args.num_steps, args.num_envs, args.n_traj)).to(device)
+    values = torch.zeros((args.num_steps, args.num_envs, args.n_traj)).to(device)
+
+    # TRY NOT TO MODIFY: start the game
+    global_step = 0
+    start_time = time.time()
+    next_obs = envs.reset()
+    next_done = torch.zeros(args.num_envs, args.n_traj).to(device)
+    num_updates = args.total_timesteps // args.batch_size
+    for update in range(1, num_updates + 1):
+        agent.train()
+        # Annealing the rate if instructed to do so.
+        if args.anneal_lr:
+            frac = 1.0 - (update - 1.0) / num_updates
+            lrnow = frac * args.learning_rate
+            optimizer.param_groups[0]["lr"] = lrnow
+        next_obs = envs.reset()
+        encoder_state = agent.backbone.encode(next_obs)
+        next_done = torch.zeros(args.num_envs, args.n_traj).to(device)
+        r = []
+        for step in range(0, args.num_steps):
+            global_step += 1 * args.num_envs
+            obs[step] = next_obs
+            dones[step] = next_done
+
+            # ALGO LOGIC: action logic
+            with torch.no_grad():
+                action, logprob, _, value, _ = agent.get_action_and_value_cached(
+                    next_obs, state=encoder_state
+                )
+                action = action.view(args.num_envs, args.n_traj)
+                values[step] = value.view(args.num_envs, args.n_traj)
+            actions[step] = action
+            logprobs[step] = logprob.view(args.num_envs, args.n_traj)
+            # TRY NOT TO MODIFY: execute the game and log data.
+            next_obs, reward, done, info = envs.step(action.cpu().numpy())
+            rewards[step] = torch.tensor(reward).to(device)
+            next_obs, next_done = next_obs, torch.Tensor(done).to(device)
+
+            for item in info:
+                if "episode" in item.keys():
+                    r.append(item)
+        print("completed_episodes=", len(r))
+        avg_episodic_return = np.mean([rollout["episode"]["r"].mean() for rollout in r])
+        max_episodic_return = np.mean([rollout["episode"]["r"].max() for rollout in r])
+        avg_episodic_length = np.mean([rollout["episode"]["l"].mean() for rollout in r])
+        print(
+            f"[Train] global_step={global_step}\n \
+            avg_episodic_return={avg_episodic_return}\n \
+            max_episodic_return={max_episodic_return}\n \
+            avg_episodic_length={avg_episodic_length}"
+        )
+        writer.add_scalar("charts/episodic_return_mean", avg_episodic_return, global_step)
+        writer.add_scalar("charts/episodic_return_max", max_episodic_return, global_step)
+        writer.add_scalar("charts/episodic_length", avg_episodic_length, global_step)
+        # bootstrap value if not done
+        with torch.no_grad():
+            next_value = agent.get_value_cached(next_obs, encoder_state).squeeze(-1)  # B x T
+            advantages = torch.zeros_like(rewards).to(device)  # steps x B x T
+            lastgaelam = torch.zeros(args.num_envs, args.n_traj).to(device)  # B x T
+            for t in reversed(range(args.num_steps)):
+                if t == args.num_steps - 1:
+                    nextnonterminal = 1.0 - next_done  # next_done: B
+                    nextvalues = next_value  # B x T
+                else:
+                    nextnonterminal = 1.0 - dones[t + 1]
+                    nextvalues = values[t + 1]  # B x T
+                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
+                advantages[t] = lastgaelam = (
+                    delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
+                )
+            returns = advantages + values
+
+        # flatten the batch
+        b_obs = {
+            k: np.concatenate([obs_[k] for obs_ in obs]) for k in envs.single_observation_space
+        }
+
+        # Edited
+        b_logprobs = logprobs.reshape(-1, args.n_traj)
+        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
+        b_advantages = advantages.reshape(-1, args.n_traj)
+        b_returns = returns.reshape(-1, args.n_traj)
+        b_values = values.reshape(-1, args.n_traj)
+
+        # Optimizing the policy and value network
+        assert args.num_envs % args.num_minibatches == 0
+        envsperbatch = args.num_envs // args.num_minibatches
+        envinds = np.arange(args.num_envs)
+        flatinds = np.arange(args.batch_size).reshape(args.num_steps, args.num_envs)
+
+        clipfracs = []
+        for epoch in range(args.update_epochs):
+            np.random.shuffle(envinds)
+            for start in range(0, args.num_envs, envsperbatch):
+                end = start + envsperbatch
+                mbenvinds = envinds[start:end]  # mini batch env id
+                mb_inds = flatinds[:, mbenvinds].ravel()  # be really careful about the index
+                r_inds = np.tile(np.arange(envsperbatch), args.num_steps)
+
+                cur_obs = {k: v[mbenvinds] for k, v in obs[0].items()}
+                encoder_state = agent.backbone.encode(cur_obs)
+                _, newlogprob, entropy, newvalue, _ = agent.get_action_and_value_cached(
+                    {k: v[mb_inds] for k, v in b_obs.items()},
+                    b_actions.long()[mb_inds],
+                    (embedding[r_inds, :] for embedding in encoder_state),
+                )
+                # _, newlogprob, entropy, newvalue = agent.get_action_and_value({k:v[mb_inds] for k,v in b_obs.items()}, b_actions.long()[mb_inds])
+                logratio = newlogprob - b_logprobs[mb_inds]
+                ratio = logratio.exp()
+
+                with torch.no_grad():
+                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
+                    old_approx_kl = (-logratio).mean()
+                    approx_kl = ((ratio - 1) - logratio).mean()
+                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
+
+                mb_advantages = b_advantages[mb_inds]
+                if args.norm_adv:
+                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (
+                        mb_advantages.std() + 1e-8
+                    )
+
+                # Policy loss
+                pg_loss1 = -mb_advantages * ratio
+                pg_loss2 = -mb_advantages * torch.clamp(
+                    ratio, 1 - args.clip_coef, 1 + args.clip_coef
+                )
+                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
+                # Value loss
+                newvalue = newvalue.view(-1, args.n_traj)
+                if args.clip_vloss:
+                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
+                    v_clipped = b_values[mb_inds] + torch.clamp(
+                        newvalue - b_values[mb_inds],
+                        -args.clip_coef,
+                        args.clip_coef,
+                    )
+                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
+                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
+                    v_loss = 0.5 * v_loss_max.mean()
+                else:
+                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
+
+                entropy_loss = entropy.mean()
+                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
+
+                optimizer.zero_grad()
+                loss.backward()
+                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
+                optimizer.step()
+
+            if args.target_kl is not None:
+                if approx_kl > args.target_kl:
+                    break
+
+        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
+        var_y = np.var(y_true)
+        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
+
+        # TRY NOT TO MODIFY: record rewards for plotting purposes
+        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
+        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
+        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
+        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
+        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
+        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
+        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
+        writer.add_scalar("losses/explained_variance", explained_var, global_step)
+        print("SPS:", int(global_step / (time.time() - start_time)))
+        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
+        if update % 1000 == 0 or update == num_updates:
+            torch.save(agent.state_dict(), f"runs/{run_name}/ckpt/{update}.pt")
+        if update % 100 == 0 or update == num_updates:
+            agent.eval()
+            test_obs = test_envs.reset()
+            r = []
+            for step in range(0, args.num_steps):
+                # ALGO LOGIC: action logic
+                with torch.no_grad():
+                    action, logits = agent(test_obs)
+                if step == 0:
+                    if args.multi_greedy_inference:
+                        if args.problem == 'tsp':
+                            action = torch.arange(args.n_traj).repeat(args.n_test, 1)
+                        elif args.problem == 'cvrp':
+                            action = torch.arange(1, args.n_traj + 1).repeat(args.n_test, 1)
+                # TRY NOT TO MODIFY: execute the game and log data.
+                test_obs, _, _, test_info = test_envs.step(action.cpu().numpy())
+
+                for item in test_info:
+                    if "episode" in item.keys():
+                        r.append(item)
+
+            avg_episodic_return = np.mean([rollout["episode"]["r"].mean() for rollout in r])
+            max_episodic_return = np.mean([rollout["episode"]["r"].max() for rollout in r])
+            avg_episodic_length = np.mean([rollout["episode"]["l"].mean() for rollout in r])
+            print(f"[test] episodic_return={max_episodic_return}")
+            writer.add_scalar("test/episodic_return_mean", avg_episodic_return, global_step)
+            writer.add_scalar("test/episodic_return_max", max_episodic_return, global_step)
+            writer.add_scalar("test/episodic_length", avg_episodic_length, global_step)
+
+    envs.close()
+    writer.close()
diff --git a/runs/cvrp-v1__exp5_50_steps___1__1711112859/events.out.tfevents.1711112903.VF-070237N.18980.0 b/runs/cvrp-v1__exp5_50_steps___1__1711112859/events.out.tfevents.1711112903.VF-070237N.18980.0
new file mode 100644
index 0000000..d9c5a7e
Binary files /dev/null and b/runs/cvrp-v1__exp5_50_steps___1__1711112859/events.out.tfevents.1711112903.VF-070237N.18980.0 differ
diff --git a/runs/cvrp-v1__exp5_50_steps___1__1711112859/main.py b/runs/cvrp-v1__exp5_50_steps___1__1711112859/main.py
new file mode 100644
index 0000000..b396b35
--- /dev/null
+++ b/runs/cvrp-v1__exp5_50_steps___1__1711112859/main.py
@@ -0,0 +1,397 @@
+# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppopy
+
+import argparse
+import os
+import random
+import shutil
+import time
+from distutils.util import strtobool
+
+import gym
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.optim as optim
+from torch.utils.tensorboard import SummaryWriter
+
+
+def parse_args():
+    # fmt: off
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
+        help="the name of this experiment")
+    parser.add_argument("--seed", type=int, default=1,
+        help="seed of the experiment")
+    parser.add_argument("--torch-deterministic", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="if toggled, `torch.backends.cudnn.deterministic=False`")
+    parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="if toggled, cuda will be enabled by default")
+    parser.add_argument("--track", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
+        help="if toggled, this experiment will be tracked with Weights and Biases")
+    parser.add_argument("--wandb-project-name", type=str, default="cleanRL",
+        help="the wandb's project name")
+    parser.add_argument("--wandb-entity", type=str, default=None,
+        help="the entity (team) of wandb's project")
+    parser.add_argument("--capture-video", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
+        help="whether to capture videos of the agent performances (check out `videos` folder)")
+
+    # Algorithm specific arguments
+    parser.add_argument("--problem", type=str, default="cvrp_fleet",
+        help="the OR problem we are trying to solve, it will be passed to the agent")
+    parser.add_argument("--env-id", type=str, default="cvrp-v1",
+        help="the id of the environment")
+    parser.add_argument("--env-entry-point", type=str, default="envs.cvrp_vehfleet_env:CVRPFleetEnv",
+        help="the path to the definition of the environment, for example `envs.cvrp_vector_env:CVRPVectorEnv` if the `CVRPVectorEnv` class is defined in ./envs/cvrp_vector_env.py")
+    parser.add_argument("--total-timesteps", type=int, default=6_000_000_000,
+        help="total timesteps of the experiments")
+    parser.add_argument("--learning-rate", type=float, default=1e-3,
+        help="the learning rate of the optimizer")
+    parser.add_argument("--weight-decay", type=float, default=0,
+        help="the weight decay of the optimizer")
+    parser.add_argument("--num-envs", type=int, default=1024,
+        help="the number of parallel game environments")
+    parser.add_argument("--num-steps", type=int, default=100,
+        help="the number of steps to run in each environment per policy rollout")
+    parser.add_argument("--anneal-lr", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="Toggle learning rate annealing for policy and value networks")
+    parser.add_argument("--gamma", type=float, default=0.99,
+        help="the discount factor gamma")
+    parser.add_argument("--gae-lambda", type=float, default=0.95,
+        help="the lambda for the general advantage estimation")
+    parser.add_argument("--num-minibatches", type=int, default=8,
+        help="the number of mini-batches")
+    parser.add_argument("--update-epochs", type=int, default=2,
+        help="the K epochs to update the policy")
+    parser.add_argument("--norm-adv", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="Toggles advantages normalization")
+    parser.add_argument("--clip-coef", type=float, default=0.2,
+        help="the surrogate clipping coefficient")
+    parser.add_argument("--clip-vloss", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="Toggles whether or not to use a clipped loss for the value function, as per the paper.")
+    parser.add_argument("--ent-coef", type=float, default=0.01,
+        help="coefficient of the entropy")
+    parser.add_argument("--vf-coef", type=float, default=0.5,
+        help="coefficient of the value function")
+    parser.add_argument("--max-grad-norm", type=float, default=0.5,
+        help="the maximum norm for the gradient clipping")
+    parser.add_argument("--target-kl", type=float, default=None,
+        help="the target KL divergence threshold")
+    parser.add_argument("--n-traj", type=int, default=50,
+        help="number of trajectories in a vectorized sub-environment")
+    parser.add_argument("--n-test", type=int, default=1000,
+        help="how many test instance")
+    parser.add_argument("--multi-greedy-inference", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
+        help="whether to use multiple trajectory greedy inference")
+    args = parser.parse_args()
+    args.batch_size = int(args.num_envs * args.num_steps)
+    args.minibatch_size = int(args.batch_size // args.num_minibatches)
+    # fmt: on
+    return args
+
+
+from wrappers.recordWrapper import RecordEpisodeStatistics
+
+
+def make_env(env_id, seed, cfg={}):
+    def thunk():
+        env = gym.make(env_id, **cfg)
+        env = RecordEpisodeStatistics(env)
+        env.seed(seed)
+        env.action_space.seed(seed)
+        env.observation_space.seed(seed)
+        return env
+
+    return thunk
+
+
+from models.attention_model_wrapper import Agent
+from wrappers.syncVectorEnvPomo import SyncVectorEnv
+
+if __name__ == "__main__":
+    args = parse_args()
+    run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
+    if args.track:
+        import wandb
+
+        wandb.init(
+            project=args.wandb_project_name,
+            entity=args.wandb_entity,
+            sync_tensorboard=True,
+            config=vars(args),
+            name=run_name,
+            monitor_gym=True,
+            save_code=True,
+        )
+    writer = SummaryWriter(f"runs/{run_name}")
+    writer.add_text(
+        "hyperparameters",
+        "|param|value|\n|-|-|\n%s"
+        % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
+    )
+    os.makedirs(os.path.join(f"runs/{run_name}", "ckpt"), exist_ok=True)
+    shutil.copy(__file__, os.path.join(f"runs/{run_name}", "main.py"))
+    # TRY NOT TO MODIFY: seeding
+    random.seed(args.seed)
+    np.random.seed(args.seed)
+    torch.manual_seed(args.seed)
+    torch.backends.cudnn.deterministic = args.torch_deterministic
+
+    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
+
+    #######################
+    #### Env defintion ####
+    #######################
+
+    gym.envs.register(
+        id=args.env_id,
+        entry_point=args.env_entry_point,
+    )
+
+    # training env setup
+    envs = SyncVectorEnv([make_env(args.env_id, args.seed + i) for i in range(args.num_envs)])
+    # evaluation env setup: 1.) from a fix dataset, or 2.) generated with seed
+   # 1.) use test instance from a fix dataset
+    test_envs = SyncVectorEnv(
+        [
+            make_env(
+                args.env_id,
+                args.seed + i,
+                cfg={"eval_data": True, "eval_partition": "eval", "eval_data_idx": i},
+            )
+            for i in range(args.n_test)
+        ]
+    )
+#     # 2.) use generated evaluation instance instead
+#     import logging
+#     logging.warning('Using generated evaluation instance. For benchmarking, please download the fix dataset.')
+#     test_envs = SyncVectorEnv([make_env(args.env_id, args.seed + args.num_envs + i) for i in range(args.n_test)])
+    
+    assert isinstance(
+        envs.single_action_space, gym.spaces.MultiDiscrete
+    ), "only discrete action space is supported"
+
+    #######################
+    ### Agent defintion ###
+    #######################
+
+    agent = Agent(device=device, name=args.problem).to(device)
+    # agent.backbone.load_state_dict(torch.load('./vrp50.pt'))
+    optimizer = optim.Adam(
+        agent.parameters(), lr=args.learning_rate, eps=1e-5, weight_decay=args.weight_decay
+    )
+
+    #######################
+    # Algorithm defintion #
+    #######################
+
+    # ALGO Logic: Storage setup
+    obs = [None] * args.num_steps
+    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(
+        device
+    )
+    logprobs = torch.zeros((args.num_steps, args.num_envs, args.n_traj)).to(device)
+    rewards = torch.zeros((args.num_steps, args.num_envs, args.n_traj)).to(device)
+    dones = torch.zeros((args.num_steps, args.num_envs, args.n_traj)).to(device)
+    values = torch.zeros((args.num_steps, args.num_envs, args.n_traj)).to(device)
+
+    # TRY NOT TO MODIFY: start the game
+    global_step = 0
+    start_time = time.time()
+    next_obs = envs.reset()
+    next_done = torch.zeros(args.num_envs, args.n_traj).to(device)
+    num_updates = args.total_timesteps // args.batch_size
+    for update in range(1, num_updates + 1):
+        agent.train()
+        # Annealing the rate if instructed to do so.
+        if args.anneal_lr:
+            frac = 1.0 - (update - 1.0) / num_updates
+            lrnow = frac * args.learning_rate
+            optimizer.param_groups[0]["lr"] = lrnow
+        next_obs = envs.reset()
+        encoder_state = agent.backbone.encode(next_obs)
+        next_done = torch.zeros(args.num_envs, args.n_traj).to(device)
+        r = []
+        for step in range(0, args.num_steps):
+            global_step += 1 * args.num_envs
+            obs[step] = next_obs
+            dones[step] = next_done
+
+            # ALGO LOGIC: action logic
+            with torch.no_grad():
+                action, logprob, _, value, _ = agent.get_action_and_value_cached(
+                    next_obs, state=encoder_state
+                )
+                action = action.view(args.num_envs, args.n_traj)
+                values[step] = value.view(args.num_envs, args.n_traj)
+            actions[step] = action
+            logprobs[step] = logprob.view(args.num_envs, args.n_traj)
+            # TRY NOT TO MODIFY: execute the game and log data.
+            next_obs, reward, done, info = envs.step(action.cpu().numpy())
+            rewards[step] = torch.tensor(reward).to(device)
+            next_obs, next_done = next_obs, torch.Tensor(done).to(device)
+
+            for item in info:
+                if "episode" in item.keys():
+                    r.append(item)
+        print("completed_episodes=", len(r))
+        avg_episodic_return = np.mean([rollout["episode"]["r"].mean() for rollout in r])
+        max_episodic_return = np.mean([rollout["episode"]["r"].max() for rollout in r])
+        avg_episodic_length = np.mean([rollout["episode"]["l"].mean() for rollout in r])
+        print(
+            f"[Train] global_step={global_step}\n \
+            avg_episodic_return={avg_episodic_return}\n \
+            max_episodic_return={max_episodic_return}\n \
+            avg_episodic_length={avg_episodic_length}"
+        )
+        writer.add_scalar("charts/episodic_return_mean", avg_episodic_return, global_step)
+        writer.add_scalar("charts/episodic_return_max", max_episodic_return, global_step)
+        writer.add_scalar("charts/episodic_length", avg_episodic_length, global_step)
+        # bootstrap value if not done
+        with torch.no_grad():
+            next_value = agent.get_value_cached(next_obs, encoder_state).squeeze(-1)  # B x T
+            advantages = torch.zeros_like(rewards).to(device)  # steps x B x T
+            lastgaelam = torch.zeros(args.num_envs, args.n_traj).to(device)  # B x T
+            for t in reversed(range(args.num_steps)):
+                if t == args.num_steps - 1:
+                    nextnonterminal = 1.0 - next_done  # next_done: B
+                    nextvalues = next_value  # B x T
+                else:
+                    nextnonterminal = 1.0 - dones[t + 1]
+                    nextvalues = values[t + 1]  # B x T
+                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]
+                advantages[t] = lastgaelam = (
+                    delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam
+                )
+            returns = advantages + values
+
+        # flatten the batch
+        b_obs = {
+            k: np.concatenate([obs_[k] for obs_ in obs]) for k in envs.single_observation_space
+        }
+
+        # Edited
+        b_logprobs = logprobs.reshape(-1, args.n_traj)
+        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
+        b_advantages = advantages.reshape(-1, args.n_traj)
+        b_returns = returns.reshape(-1, args.n_traj)
+        b_values = values.reshape(-1, args.n_traj)
+
+        # Optimizing the policy and value network
+        assert args.num_envs % args.num_minibatches == 0
+        envsperbatch = args.num_envs // args.num_minibatches
+        envinds = np.arange(args.num_envs)
+        flatinds = np.arange(args.batch_size).reshape(args.num_steps, args.num_envs)
+
+        clipfracs = []
+        for epoch in range(args.update_epochs):
+            np.random.shuffle(envinds)
+            for start in range(0, args.num_envs, envsperbatch):
+                end = start + envsperbatch
+                mbenvinds = envinds[start:end]  # mini batch env id
+                mb_inds = flatinds[:, mbenvinds].ravel()  # be really careful about the index
+                r_inds = np.tile(np.arange(envsperbatch), args.num_steps)
+
+                cur_obs = {k: v[mbenvinds] for k, v in obs[0].items()}
+                encoder_state = agent.backbone.encode(cur_obs)
+                _, newlogprob, entropy, newvalue, _ = agent.get_action_and_value_cached(
+                    {k: v[mb_inds] for k, v in b_obs.items()},
+                    b_actions.long()[mb_inds],
+                    (embedding[r_inds, :] for embedding in encoder_state),
+                )
+                # _, newlogprob, entropy, newvalue = agent.get_action_and_value({k:v[mb_inds] for k,v in b_obs.items()}, b_actions.long()[mb_inds])
+                logratio = newlogprob - b_logprobs[mb_inds]
+                ratio = logratio.exp()
+
+                with torch.no_grad():
+                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
+                    old_approx_kl = (-logratio).mean()
+                    approx_kl = ((ratio - 1) - logratio).mean()
+                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
+
+                mb_advantages = b_advantages[mb_inds]
+                if args.norm_adv:
+                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (
+                        mb_advantages.std() + 1e-8
+                    )
+
+                # Policy loss
+                pg_loss1 = -mb_advantages * ratio
+                pg_loss2 = -mb_advantages * torch.clamp(
+                    ratio, 1 - args.clip_coef, 1 + args.clip_coef
+                )
+                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
+                # Value loss
+                newvalue = newvalue.view(-1, args.n_traj)
+                if args.clip_vloss:
+                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
+                    v_clipped = b_values[mb_inds] + torch.clamp(
+                        newvalue - b_values[mb_inds],
+                        -args.clip_coef,
+                        args.clip_coef,
+                    )
+                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
+                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
+                    v_loss = 0.5 * v_loss_max.mean()
+                else:
+                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
+
+                entropy_loss = entropy.mean()
+                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
+
+                optimizer.zero_grad()
+                loss.backward()
+                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
+                optimizer.step()
+
+            if args.target_kl is not None:
+                if approx_kl > args.target_kl:
+                    break
+
+        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
+        var_y = np.var(y_true)
+        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
+
+        # TRY NOT TO MODIFY: record rewards for plotting purposes
+        writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
+        writer.add_scalar("losses/value_loss", v_loss.item(), global_step)
+        writer.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
+        writer.add_scalar("losses/entropy", entropy_loss.item(), global_step)
+        writer.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
+        writer.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
+        writer.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
+        writer.add_scalar("losses/explained_variance", explained_var, global_step)
+        print("SPS:", int(global_step / (time.time() - start_time)))
+        writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
+        if update % 1000 == 0 or update == num_updates:
+            torch.save(agent.state_dict(), f"runs/{run_name}/ckpt/{update}.pt")
+        if update % 100 == 0 or update == num_updates:
+            agent.eval()
+            test_obs = test_envs.reset()
+            r = []
+            for step in range(0, args.num_steps):
+                # ALGO LOGIC: action logic
+                with torch.no_grad():
+                    action, logits = agent(test_obs)
+                if step == 0:
+                    if args.multi_greedy_inference:
+                        if args.problem == 'tsp':
+                            action = torch.arange(args.n_traj).repeat(args.n_test, 1)
+                        elif args.problem == 'cvrp':
+                            action = torch.arange(1, args.n_traj + 1).repeat(args.n_test, 1)
+                # TRY NOT TO MODIFY: execute the game and log data.
+                test_obs, _, _, test_info = test_envs.step(action.cpu().numpy())
+
+                for item in test_info:
+                    if "episode" in item.keys():
+                        r.append(item)
+
+            avg_episodic_return = np.mean([rollout["episode"]["r"].mean() for rollout in r])
+            max_episodic_return = np.mean([rollout["episode"]["r"].max() for rollout in r])
+            avg_episodic_length = np.mean([rollout["episode"]["l"].mean() for rollout in r])
+            print(f"[test] episodic_return={max_episodic_return}")
+            writer.add_scalar("test/episodic_return_mean", avg_episodic_return, global_step)
+            writer.add_scalar("test/episodic_return_max", max_episodic_return, global_step)
+            writer.add_scalar("test/episodic_length", avg_episodic_length, global_step)
+
+    envs.close()
+    writer.close()
diff --git a/wandb/debug-cli.dein_el.log b/wandb/debug-cli.dein_el.log
new file mode 100644
index 0000000..e69de29
